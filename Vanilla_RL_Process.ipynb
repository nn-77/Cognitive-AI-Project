{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from Agent import NeuralAgent, plot_return\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# All gymnasium environments follow an API format that ensures they are each fully featured, self sufficient, and all essentially the same (looking at the environment from a black-box perspective)\n",
    "\n",
    "# An environment has a make method, a reset method, a step method, and a render method. In a typical loop, you use the make() method to create the environment, reset() to set and return the initial state of the environment, then repeat:\n",
    "    # determine which action the agent should take next (the reinforcement learning component)\n",
    "    # pass this action into the step() method to advance the environment\n",
    "        # the render is automatically displayed by step() if render_mode is set to 'human'\n",
    "# you can reset() and repeat this loop multiple times using the same initialized environment (made with make()).\n",
    "\n",
    "# every environment also has an action_space attribute and an observation_space attribute. The action_space attribute defines the possible actions that can be taken in this environment, defining the output of our RL network. The observation_space attribute defines the format of the state space (i.e. what the input will look like for our RL network)"
   ],
   "id": "3abc7a5a46d6d669",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# for example, the code in this cell spins up the game we are \"learning\" in this project, and just takes random actions at each time step:\n",
    "\n",
    "run_sample = False\n",
    "if run_sample: # so the sample code does not run\n",
    "    sample_env = gym.make(\"ALE/Berzerk-v5\", render_mode=\"human\")\n",
    "    sample_env.reset()\n",
    "    \n",
    "    for _ in range(1000):\n",
    "        sample_action = sample_env.action_space.sample()  # agent policy that uses the observation and info\n",
    "        _, _, sample_terminated, sample_truncated, _ = sample_env.step(sample_action)\n",
    "    \n",
    "        if sample_terminated or sample_truncated:\n",
    "            observation, info = sample_env.reset()\n",
    "    \n",
    "    sample_env.close()"
   ],
   "id": "c868c1b7facf6853",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Vanilla Agent Implementation",
   "id": "8c9c12e9909960dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Our main goal is to train an instance of a vanilla RL agent (implementation in Agent.py) to play this game, and evaluate its execution",
   "id": "a4d9770982185b69",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# wrapper helper\n",
    "class ReduceActionSpace(gym.Wrapper):\n",
    "    def __init__(self, env, actions):\n",
    "        super().__init__(env)\n",
    "        self.action_map = actions\n",
    "        # Define the new action space\n",
    "        self.action_space = spaces.Discrete(len(actions))\n",
    "\n",
    "    def step(self, action):\n",
    "        # Map the action from the reduced space to the original space\n",
    "        action = self.action_map[action]\n",
    "        return self.env.step(action)"
   ],
   "id": "ac22fe87be988517",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "do_lunar = False\n",
    "if do_lunar:\n",
    "    ###################################################################################################\n",
    "    # Initialize environment and extract all relevant components\n",
    "    \n",
    "    train_env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")\n",
    "    \n",
    "    # assuming discrete action space and 1-dimensional observation space\n",
    "    ACTION_SPACE_SIZE = train_env.action_space.n\n",
    "    print(f\"Action space size: {ACTION_SPACE_SIZE}\")\n",
    "    OBSERVATION_SPACE_SIZE = train_env.observation_space.shape[0]\n",
    "    print(f\"Observation space size: {OBSERVATION_SPACE_SIZE}\")\n",
    "    \n",
    "    #################################################################################################\n",
    "    # Initialize agent\n",
    "    \n",
    "    folder_to_save_checkpoints = \"./lunarVALIDATE_checkpoints\"\n",
    "    # parameter settings for v5\n",
    "    gamma = 0.99\n",
    "    lr = .00025\n",
    "    max_storage_size = 25000\n",
    "    batch_size = 64\n",
    "    exploration_rate_decay = 0.99999\n",
    "    agent = NeuralAgent(OBSERVATION_SPACE_SIZE, ACTION_SPACE_SIZE, folder_to_save_checkpoints, gamma, lr, max_storage_size, batch_size, exploration_rate_decay)\n",
    "    \n",
    "    #################################################################################################\n",
    "    # Load model into agent if desired\n",
    "    \n",
    "    load_agent = True\n",
    "    load_file = \"./lunarV5_checkpoints/04-21_14-10-27_neural_agent_2.pth\"\n",
    "    \n",
    "    if load_agent:\n",
    "        agent.load(load_file)\n",
    "    \n",
    "    ########################################################################################################\n",
    "    # Train the agent if desired. Will display metrics per batch of episodes, as well as a plot of returns for each episode when training is done\n",
    "    \n",
    "    perform_training = False\n",
    "    \n",
    "    if perform_training:\n",
    "        print(f\"Device: {agent.device}\\nLearning Rate: {agent.lr}\\nBatch Size: {agent.batch_size}\\nExploration Rate Decay: {agent.exploration_rate_decay}\\nMaximum Memory Size: {agent.max_storage_size}\")\n",
    "        np_filename = agent.simulate(train_env, 1000)\n",
    "        plot_return(np_filename)\n",
    "    \n",
    "    train_env.close()\n",
    "    ########################################################################################################\n",
    "    # Inject the vanilla RL agent into the gymnasium render loop, and visualize (in a human way) how well it plays the game:\n",
    "    \n",
    "    # Lunar V5 performed the best (parameters as above) - solved the game!\n",
    "        # important part was making the memory size small\n",
    "    \n",
    "    do_render = True\n",
    "    \n",
    "    if do_render:\n",
    "        env_human = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "        agent.render_agent_game(env_human, 2000)"
   ],
   "id": "2df99cda054ce558",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "do_pong = False\n",
    "if do_pong:\n",
    "    ###################################################################################################\n",
    "    # Initialize environment and extract all relevant components\n",
    "    \n",
    "    train_env = gym.make(\"ALE/Pong-v5\", repeat_action_probability=0, obs_type=\"ram\", render_mode=\"rgb_array\")\n",
    "    \n",
    "    # assuming discrete action space and 1-dimensional observation space\n",
    "    ACTION_SPACE_SIZE = train_env.action_space.n\n",
    "    print(f\"Action space size: {ACTION_SPACE_SIZE}\")\n",
    "    OBSERVATION_SPACE_SIZE = train_env.observation_space.shape[0]\n",
    "    print(f\"Observation space size: {OBSERVATION_SPACE_SIZE}\")\n",
    "    \n",
    "    #################################################################################################\n",
    "    # Initialize agent\n",
    "    \n",
    "    folder_to_save_checkpoints = \"./pongV2_checkpoints\"\n",
    "    gamma = 0.99\n",
    "    lr = .000075\n",
    "    max_storage_size = 40000\n",
    "    batch_size = 64\n",
    "    exploration_rate_decay = 0.9999994\n",
    "    agent = NeuralAgent(OBSERVATION_SPACE_SIZE, ACTION_SPACE_SIZE, folder_to_save_checkpoints, gamma, lr, max_storage_size, batch_size, exploration_rate_decay)\n",
    "    \n",
    "    #################################################################################################\n",
    "    # Load model into agent if desired\n",
    "    \n",
    "    load_agent = True\n",
    "    load_file = \"./pongV2_checkpoints/04-23_01-29-34_neural_agent_19.pth\" # 19 was good\n",
    "    \n",
    "    if load_agent:\n",
    "        agent.load(load_file)\n",
    "    \n",
    "    ########################################################################################################\n",
    "    # Train the agent if desired. Will display metrics per batch of episodes, as well as a plot of returns for each episode when training is done\n",
    "    \n",
    "    perform_training = False\n",
    "    \n",
    "    if perform_training:\n",
    "        print(f\"Device: {agent.device}\\nLearning Rate: {agent.lr}\\nBatch Size: {agent.batch_size}\\nExploration Rate Decay: {agent.exploration_rate_decay}\\nMaximum Memory Size: {agent.max_storage_size}\")\n",
    "        np_filename = agent.simulate(train_env, 6000)  # for pong, one episode goes till the score hits 21 for either side - much longer episodes\n",
    "        plot_return(np_filename)\n",
    "    \n",
    "    train_env.close()\n",
    "    ########################################################################################################\n",
    "    # Inject the vanilla RL agent into the gymnasium render loop, and visualize (in a human way) how well it plays the game:\n",
    "    \n",
    "    do_render = True\n",
    "    \n",
    "    if do_render:\n",
    "        env_human = gym.make(\"ALE/Pong-v5\", repeat_action_probability=0, obs_type=\"ram\", render_mode=\"human\")\n",
    "        agent.render_agent_game(env_human, 2000)"
   ],
   "id": "545cc8da112d2d4d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "do_beam = False\n",
    "if do_beam:\n",
    "    ###################################################################################################\n",
    "    # Initialize environment and extract all relevant components\n",
    "    \n",
    "    train_env = gym.make(\"ALE/BeamRider-v5\", repeat_action_probability=0, obs_type=\"ram\", render_mode=\"rgb_array\")\n",
    "    CUSTOM_ACTION_SPACE = [1, 3, 4]\n",
    "    train_env = ReduceActionSpace(train_env, CUSTOM_ACTION_SPACE)\n",
    "    \n",
    "    # assuming discrete action space and 1-dimensional observation space\n",
    "    ACTION_SPACE_SIZE = train_env.action_space.n\n",
    "    print(f\"Action space size: {ACTION_SPACE_SIZE}\")\n",
    "    OBSERVATION_SPACE_SIZE = train_env.observation_space.shape[0]\n",
    "    print(f\"Observation space size: {OBSERVATION_SPACE_SIZE}\")\n",
    "    \n",
    "    #################################################################################################\n",
    "    # Initialize agent\n",
    "    \n",
    "    folder_to_save_checkpoints = \"./beamV2_checkpoints\"\n",
    "    gamma = 0.99\n",
    "    lr = .00005\n",
    "    max_storage_size = 65000\n",
    "    batch_size = 64\n",
    "    exploration_rate_decay = 0.999995\n",
    "    agent = NeuralAgent(OBSERVATION_SPACE_SIZE, ACTION_SPACE_SIZE, folder_to_save_checkpoints, gamma, lr, max_storage_size, batch_size, exploration_rate_decay)\n",
    "    \n",
    "    #################################################################################################\n",
    "    # Load model into agent if desired\n",
    "    \n",
    "    load_agent = False\n",
    "    load_file = \"\"\n",
    "    \n",
    "    if load_agent:\n",
    "        agent.load(load_file)\n",
    "    \n",
    "    ########################################################################################################\n",
    "    # Train the agent if desired. Will display metrics per batch of episodes, as well as a plot of returns for each episode when training is done\n",
    "    \n",
    "    perform_training = True\n",
    "    \n",
    "    if perform_training:\n",
    "        print(f\"Device: {agent.device}\\nLearning Rate: {agent.lr}\\nBatch Size: {agent.batch_size}\\nExploration Rate Decay: {agent.exploration_rate_decay}\\nMaximum Memory Size: {agent.max_storage_size}\")\n",
    "        np_filename = agent.simulate(train_env, 15000)\n",
    "        plot_return(np_filename)\n",
    "    \n",
    "    train_env.close()\n",
    "    ########################################################################################################\n",
    "    # Inject the vanilla RL agent into the gymnasium render loop, and visualize (in a human way) how well it plays the game:\n",
    "    \n",
    "    do_render = False\n",
    "    \n",
    "    if do_render:\n",
    "        env_human = gym.make(\"ALE/BeamRider-v5\", repeat_action_probability=0, obs_type=\"ram\", render_mode=\"human\")\n",
    "        env_human = ReduceActionSpace(env_human, CUSTOM_ACTION_SPACE)\n",
    "        agent.render_agent_game(env_human, 2000)"
   ],
   "id": "da8b88a2ea10ac44",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "do_chicken = True\n",
    "if do_chicken:\n",
    "    ###################################################################################################\n",
    "    # Initialize environment and extract all relevant components\n",
    "    \n",
    "    train_env = gym.make(\"ALE/Freeway-v5\", repeat_action_probability=0, obs_type=\"ram\", render_mode=\"rgb_array\")\n",
    "    \n",
    "    # assuming discrete action space and 1-dimensional observation space\n",
    "    ACTION_SPACE_SIZE = train_env.action_space.n\n",
    "    print(f\"Action space size: {ACTION_SPACE_SIZE}\")\n",
    "    OBSERVATION_SPACE_SIZE = train_env.observation_space.shape[0]\n",
    "    print(f\"Observation space size: {OBSERVATION_SPACE_SIZE}\")\n",
    "    \n",
    "    #################################################################################################\n",
    "    # Initialize agent\n",
    "    \n",
    "    folder_to_save_checkpoints = \"./chickenVVERIFY_checkpoints\"\n",
    "    gamma = 0.99\n",
    "    lr = .00005\n",
    "    max_storage_size = 50000\n",
    "    batch_size = 64\n",
    "    exploration_rate_decay = 0.9999935\n",
    "    agent = NeuralAgent(OBSERVATION_SPACE_SIZE, ACTION_SPACE_SIZE, folder_to_save_checkpoints, gamma, lr, max_storage_size, batch_size, exploration_rate_decay)\n",
    "    \n",
    "    #################################################################################################\n",
    "    # Load model into agent if desired\n",
    "    \n",
    "    load_agent = False\n",
    "    load_file = \"./chickenV2_checkpoints/04-24_08-57-24_neural_agent_34.pth\"\n",
    "    \n",
    "    if load_agent:\n",
    "        agent.load(load_file)\n",
    "    \n",
    "    ########################################################################################################\n",
    "    # Train the agent if desired. Will display metrics per batch of episodes, as well as a plot of returns for each episode when training is done\n",
    "    \n",
    "    perform_training = True\n",
    "    \n",
    "    if perform_training:\n",
    "        print(f\"Device: {agent.device}\\nLearning Rate: {agent.lr}\\nBatch Size: {agent.batch_size}\\nExploration Rate Decay: {agent.exploration_rate_decay}\\nMaximum Memory Size: {agent.max_storage_size}\")\n",
    "        np_filename = agent.simulate(train_env, 1000)\n",
    "        plot_return(np_filename)\n",
    "    \n",
    "    train_env.close()\n",
    "    ########################################################################################################\n",
    "    # Inject the vanilla RL agent into the gymnasium render loop, and visualize (in a human way) how well it plays the game:\n",
    "    \n",
    "    do_render = False\n",
    "    \n",
    "    if do_render:\n",
    "        env_human = gym.make(\"ALE/Freeway-v5\", repeat_action_probability=0, obs_type=\"ram\", render_mode=\"human\")\n",
    "        agent.render_agent_game(env_human, 2000)"
   ],
   "id": "f9b13a1161ca8f61",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "do_berzerk = False\n",
    "if do_berzerk:\n",
    "    ###################################################################################################\n",
    "    # Initialize environment and extract all relevant components\n",
    "    \n",
    "    train_env = gym.make(\"ALE/Berzerk-v5\", mode=1, repeat_action_probability=0, obs_type=\"ram\", render_mode=\"rgb_array\")\n",
    "    CUSTOM_ACTION_SPACE = [1, 2, 3, 4, 5]\n",
    "    train_env = ReduceActionSpace(train_env, CUSTOM_ACTION_SPACE)\n",
    "    \n",
    "    # assuming discrete action space and 1-dimensional observation space\n",
    "    ACTION_SPACE_SIZE = train_env.action_space.n\n",
    "    print(f\"Action space size: {ACTION_SPACE_SIZE}\")\n",
    "    OBSERVATION_SPACE_SIZE = train_env.observation_space.shape[0]\n",
    "    print(f\"Observation space size: {OBSERVATION_SPACE_SIZE}\")\n",
    "    \n",
    "    #################################################################################################\n",
    "    # Initialize agent\n",
    "    \n",
    "    folder_to_save_checkpoints = \"./berzerkV4_checkpoints\"\n",
    "    # v3 parameter settings, along w custom actions and 85000 episodes\n",
    "    gamma = 0.99\n",
    "    lr = .000001\n",
    "    max_storage_size = 85000\n",
    "    batch_size = 64\n",
    "    exploration_rate_decay = 0.99999975\n",
    "    agent = NeuralAgent(OBSERVATION_SPACE_SIZE, ACTION_SPACE_SIZE, folder_to_save_checkpoints, gamma, lr, max_storage_size, batch_size, exploration_rate_decay)\n",
    "    \n",
    "    #################################################################################################\n",
    "    # Load model into agent if desired\n",
    "    \n",
    "    load_agent = True\n",
    "    load_file = \"./berzerkV3_checkpoints/04-22_07-51-24_neural_agent_30.pth\"\n",
    "    \n",
    "    if load_agent:\n",
    "        agent.load(load_file)\n",
    "    \n",
    "    ########################################################################################################\n",
    "    # Train the agent if desired. Will display metrics per batch of episodes, as well as a plot of returns for each episode when training is done\n",
    "    \n",
    "    perform_training = False\n",
    "    \n",
    "    if perform_training:\n",
    "        print(f\"Device: {agent.device}\\nLearning Rate: {agent.lr}\\nBatch Size: {agent.batch_size}\\nExploration Rate Decay: {agent.exploration_rate_decay}\\nMaximum Memory Size: {agent.max_storage_size}\")\n",
    "        np_filename = agent.simulate(train_env, 85000)\n",
    "        plot_return(np_filename)\n",
    "    \n",
    "    train_env.close()\n",
    "    ########################################################################################################\n",
    "    # Inject the vanilla RL agent into the gymnasium render loop, and visualize (in a human way) how well it plays the game:\n",
    "    \n",
    "    do_render = True\n",
    "    \n",
    "    if do_render:\n",
    "        env_human = gym.make(\"ALE/Berzerk-v5\", mode=1, repeat_action_probability=0, obs_type=\"ram\", render_mode=\"human\")\n",
    "        env_human = ReduceActionSpace(env_human, CUSTOM_ACTION_SPACE)\n",
    "        agent.render_agent_game(env_human, 2000)"
   ],
   "id": "516afa7066bc3fc7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # code to make sure the \"hack\" to get the rgb render of the screen when render_mode is \"human\" works - put in paper\n",
    "# \n",
    "# sample_env = gym.make(\"ALE/Berzerk-v5\", obs_type=\"ram\", render_mode=\"rgb_array\")\n",
    "# sample_env.reset()\n",
    "# \n",
    "# for _ in range(13):\n",
    "#     rgbgt = sample_env.render()\n",
    "#     rgb = sample_env.unwrapped.ale.getScreenRGB()\n",
    "# \n",
    "#     fig, axes = plt.subplots(1, 2)\n",
    "#     axes[0].imshow(rgbgt)\n",
    "#     axes[1].imshow(rgb)\n",
    "#     axes[0].set_title(f\"{rgb.shape[0]}\")\n",
    "#     plt.show()\n",
    "# \n",
    "# \n",
    "#     sample_action = sample_env.action_space.sample()  # agent policy that uses the observation and info\n",
    "#     _, _, sample_terminated, sample_truncated, _ = sample_env.step(sample_action)\n",
    "# \n",
    "#     if sample_terminated or sample_truncated:\n",
    "#         observation, info = sample_env.reset()\n",
    "# \n",
    "# sample_env.close()"
   ],
   "id": "36aacf822d712c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "811fc8d1e4a8c352",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

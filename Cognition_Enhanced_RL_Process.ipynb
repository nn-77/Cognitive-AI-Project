{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-03T18:20:18.487208Z",
     "start_time": "2024-05-03T18:20:17.674434Z"
    }
   },
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from Agent import NeuralAgent, plot_return\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T18:20:18.508055Z",
     "start_time": "2024-05-03T18:20:18.495430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# wrapper helper\n",
    "class ReduceActionSpace(gym.Wrapper):\n",
    "    def __init__(self, env, actions):\n",
    "        super().__init__(env)\n",
    "        self.action_map = actions\n",
    "        # Define the new action space\n",
    "        self.action_space = spaces.Discrete(len(actions))\n",
    "\n",
    "    def step(self, action):\n",
    "        # Map the action from the reduced space to the original space\n",
    "        action = self.action_map[action]\n",
    "        return self.env.step(action)"
   ],
   "id": "62bd4e31959b6ea9",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T18:20:18.521170Z",
     "start_time": "2024-05-03T18:20:18.509636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# code to set the exploration rate to 0 and just display the frames and info shown by the \"demo\" functions in Cognitive_Hypotheses.py, if they are uncommented.\n",
    "do_show_demo_functions = False\n",
    "if do_show_demo_functions:\n",
    "    ###################################################################################################\n",
    "    # Initialize environment and extract all relevant components\n",
    "    \n",
    "    train_env = gym.make(\"ALE/Berzerk-v5\", mode=1, repeat_action_probability=0, obs_type=\"ram\", render_mode=\"rgb_array\")\n",
    "    CUSTOM_ACTION_SPACE = [1, 2, 3, 4, 5]\n",
    "    train_env = ReduceActionSpace(train_env, CUSTOM_ACTION_SPACE)\n",
    "    \n",
    "    # assuming discrete action space and 1-dimensional observation space\n",
    "    ACTION_SPACE_SIZE = train_env.action_space.n\n",
    "    print(f\"Action space size: {ACTION_SPACE_SIZE}\")\n",
    "    OBSERVATION_SPACE_SIZE = train_env.observation_space.shape[0]\n",
    "    print(f\"Observation space size: {OBSERVATION_SPACE_SIZE}\")\n",
    "    \n",
    "    #################################################################################################\n",
    "    # Initialize agent\n",
    "    \n",
    "    folder_to_save_checkpoints = \"./berzerk_show_demo\"\n",
    "    gamma = 0.99\n",
    "    lr = .000001\n",
    "    max_storage_size = 65000\n",
    "    batch_size = 64\n",
    "    exploration_rate_decay = 0.0\n",
    "    use_cognition = True\n",
    "    agent = NeuralAgent(OBSERVATION_SPACE_SIZE, ACTION_SPACE_SIZE, folder_to_save_checkpoints, gamma, lr, max_storage_size, batch_size, exploration_rate_decay, use_cognition)\n",
    "    \n",
    "    #################################################################################################\n",
    "    # Load model into agent if desired\n",
    "    \n",
    "    load_agent = False\n",
    "    load_file = \"\"\n",
    "    \n",
    "    if load_agent:\n",
    "        agent.load(load_file)\n",
    "    \n",
    "    ########################################################################################################\n",
    "    # Train the agent if desired. Will display metrics per batch of episodes, as well as a plot of returns for each episode when training is done\n",
    "    \n",
    "    perform_training = True\n",
    "    \n",
    "    if perform_training:\n",
    "        print(f\"Device: {agent.device}\\nLearning Rate: {agent.lr}\\nBatch Size: {agent.batch_size}\\nExploration Rate Decay: {agent.exploration_rate_decay}\\nMaximum Memory Size: {agent.max_storage_size}\")\n",
    "        np_filename = agent.simulate(train_env, 1000)\n",
    "        plot_return(np_filename)\n",
    "    \n",
    "    train_env.close()\n",
    "    ########################################################################################################\n",
    "    # Inject the vanilla RL agent into the gymnasium render loop, and visualize (in a human way) how well it plays the game:\n",
    "    \n",
    "    do_render = False\n",
    "    \n",
    "    if do_render:\n",
    "        env_human = gym.make(\"ALE/Berzerk-v5\", mode=1, repeat_action_probability=0, obs_type=\"ram\", render_mode=\"human\")\n",
    "        env_human = ReduceActionSpace(env_human, CUSTOM_ACTION_SPACE)\n",
    "        agent.render_agent_game(env_human, 2000)"
   ],
   "id": "1e380fe1e64be1c3",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T18:20:18.540221Z",
     "start_time": "2024-05-03T18:20:18.522990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# code to show the cognitive portion in action with a fully untrained agent\n",
    "do_show_untrained = False\n",
    "if do_show_untrained:\n",
    "    ###################################################################################################\n",
    "    # Initialize environment and extract all relevant components\n",
    "    \n",
    "    train_env = gym.make(\"ALE/Berzerk-v5\", mode=1, repeat_action_probability=0, obs_type=\"ram\", render_mode=\"rgb_array\")\n",
    "    CUSTOM_ACTION_SPACE = [1, 2, 3, 4, 5]\n",
    "    train_env = ReduceActionSpace(train_env, CUSTOM_ACTION_SPACE)\n",
    "    \n",
    "    # assuming discrete action space and 1-dimensional observation space\n",
    "    ACTION_SPACE_SIZE = train_env.action_space.n\n",
    "    print(f\"Action space size: {ACTION_SPACE_SIZE}\")\n",
    "    OBSERVATION_SPACE_SIZE = train_env.observation_space.shape[0]\n",
    "    print(f\"Observation space size: {OBSERVATION_SPACE_SIZE}\")\n",
    "    \n",
    "    #################################################################################################\n",
    "    # Initialize agent\n",
    "    \n",
    "    folder_to_save_checkpoints = \"./berzerk_show_untrained\"\n",
    "    gamma = 0.99\n",
    "    lr = .000001\n",
    "    max_storage_size = 65000\n",
    "    batch_size = 64\n",
    "    exploration_rate_decay = 0.5\n",
    "    use_cognition = True\n",
    "    agent = NeuralAgent(OBSERVATION_SPACE_SIZE, ACTION_SPACE_SIZE, folder_to_save_checkpoints, gamma, lr, max_storage_size, batch_size, exploration_rate_decay, use_cognition)\n",
    "    \n",
    "    #################################################################################################\n",
    "    # Load model into agent if desired\n",
    "    \n",
    "    load_agent = False\n",
    "    load_file = \"\"\n",
    "    \n",
    "    if load_agent:\n",
    "        agent.load(load_file)\n",
    "    \n",
    "    ########################################################################################################\n",
    "    # Train the agent if desired. Will display metrics per batch of episodes, as well as a plot of returns for each episode when training is done\n",
    "    \n",
    "    perform_training = False\n",
    "    \n",
    "    if perform_training:\n",
    "        print(f\"Device: {agent.device}\\nLearning Rate: {agent.lr}\\nBatch Size: {agent.batch_size}\\nExploration Rate Decay: {agent.exploration_rate_decay}\\nMaximum Memory Size: {agent.max_storage_size}\")\n",
    "        np_filename = agent.simulate(train_env, 1000)\n",
    "        plot_return(np_filename)\n",
    "    \n",
    "    train_env.close()\n",
    "    ########################################################################################################\n",
    "    # Inject the vanilla RL agent into the gymnasium render loop, and visualize (in a human way) how well it plays the game:\n",
    "    \n",
    "    do_render = True\n",
    "    \n",
    "    if do_render:\n",
    "        env_human = gym.make(\"ALE/Berzerk-v5\", mode=1, repeat_action_probability=0, obs_type=\"ram\", render_mode=\"human\")\n",
    "        env_human = ReduceActionSpace(env_human, CUSTOM_ACTION_SPACE)\n",
    "        agent.render_agent_game(env_human, 2000)"
   ],
   "id": "7aef77a073c96880",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-05-04T16:22:05.665158Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# code to actually train and use the cognition-enhanced agent\n",
    "do_berzerk_cognition = True\n",
    "if do_berzerk_cognition:\n",
    "    ###################################################################################################\n",
    "    # Initialize environment and extract all relevant components\n",
    "    \n",
    "    train_env = gym.make(\"ALE/Berzerk-v5\", mode=1, repeat_action_probability=0, obs_type=\"ram\", render_mode=\"rgb_array\")\n",
    "    CUSTOM_ACTION_SPACE = [1, 2, 3, 4, 5]\n",
    "    train_env = ReduceActionSpace(train_env, CUSTOM_ACTION_SPACE)\n",
    "    \n",
    "    # assuming discrete action space and 1-dimensional observation space\n",
    "    ACTION_SPACE_SIZE = train_env.action_space.n\n",
    "    print(f\"Action space size: {ACTION_SPACE_SIZE}\")\n",
    "    OBSERVATION_SPACE_SIZE = train_env.observation_space.shape[0]\n",
    "    print(f\"Observation space size: {OBSERVATION_SPACE_SIZE}\")\n",
    "    \n",
    "    #################################################################################################\n",
    "    # Initialize agent\n",
    "    \n",
    "    folder_to_save_checkpoints = \"./berzerk_COGNITION_V4\"\n",
    "    gamma = 0.75\n",
    "    lr = .00025\n",
    "    max_storage_size = 85000\n",
    "    batch_size = 64\n",
    "    exploration_rate_decay = .99999\n",
    "    use_cognition = True\n",
    "    agent = NeuralAgent(OBSERVATION_SPACE_SIZE, ACTION_SPACE_SIZE, folder_to_save_checkpoints, gamma, lr, max_storage_size, batch_size, exploration_rate_decay, use_cognition)\n",
    "    \n",
    "    #################################################################################################\n",
    "    # Load model into agent if desired\n",
    "    \n",
    "    load_agent = True\n",
    "    load_file = \"./berzerk_COGNITION_V4/05-04_12-18-53_neural_agent_35.pth\"\n",
    "    \n",
    "    if load_agent:\n",
    "        agent.load(load_file)\n",
    "    \n",
    "    ########################################################################################################\n",
    "    # Train the agent if desired. Will display metrics per batch of episodes, as well as a plot of returns for each episode when training is done\n",
    "    \n",
    "    perform_training = False\n",
    "    \n",
    "    if perform_training:\n",
    "        print(f\"Device: {agent.device}\\nLearning Rate: {agent.lr}\\nBatch Size: {agent.batch_size}\\nExploration Rate Decay: {agent.exploration_rate_decay}\\nMaximum Memory Size: {agent.max_storage_size}\")\n",
    "        np_filename = agent.simulate(train_env, 3000)\n",
    "        plot_return(np_filename)\n",
    "    \n",
    "    train_env.close()\n",
    "    ########################################################################################################\n",
    "    # Inject the vanilla RL agent into the gymnasium render loop, and visualize (in a human way) how well it plays the game:\n",
    "    \n",
    "    do_render = True\n",
    "    \n",
    "    if do_render:\n",
    "        env_human = gym.make(\"ALE/Berzerk-v5\", mode=1, repeat_action_probability=0, obs_type=\"ram\", render_mode=\"human\")\n",
    "        env_human = ReduceActionSpace(env_human, CUSTOM_ACTION_SPACE)\n",
    "        agent.render_agent_game(env_human, 2000)"
   ],
   "id": "997302b8836cb9a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# try the model in the middle that seemed to do well",
   "id": "6b0afe3a41051620"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
